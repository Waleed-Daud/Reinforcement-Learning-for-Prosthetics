{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PPO](https://arxiv.org/abs/1707.06347)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libaries (Chainer-rl, Opensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import absolute_import\n",
    "from builtins import *  # NOQA\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()  # NOQA\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import chainer\n",
    "from chainer import functions as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "# import gym\n",
    "# gym.undo_logger_setup()  # NOQA\n",
    "# import gym.wrappers\n",
    "\n",
    "\n",
    "from osim.env import ProstheticsEnv\n",
    "\n",
    "\n",
    "from chainerrl.agents import a3c\n",
    "from chainerrl.agents import PPO\n",
    "from chainerrl import experiments\n",
    "from chainerrl import links\n",
    "from chainerrl import misc\n",
    "from chainerrl.optimizers.nonbias_weight_decay import NonbiasWeightDecay\n",
    "from chainerrl import policies\n",
    "\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For all experiments we use the same random seed\n",
    "    GPU =-1 if you use CPU \n",
    "    GPU = 1 if you use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chainer's settings\n",
    "seed=1\n",
    "gpu=-1 # select CPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyper-parameters, Numbers of episodes and timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Setting\n",
    "# actor learning rate\n",
    "actor_lr=1e-4\n",
    "\n",
    "\n",
    "# other settings\n",
    "\n",
    "number_of_episodes=5000\n",
    "max_episode_length=1000 # number of time-steps per episode\n",
    "\n",
    "update_interval=4\n",
    "\n",
    "number_of_eval_runs=10\n",
    "eval_interval=10 ** 4\n",
    "\n",
    "epochs=10 ###number of epochs \n",
    "gamma=0.995 ## discont factor\n",
    "batch_size=128\n",
    "entropy_coef=0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining reward, environment, random actions, Action value functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper's functions\n",
    "\n",
    "# Linearly decay the learning rate to zero\n",
    "def lr_setter(env, agent, value):\n",
    "    agent.optimizer.alpha = value\n",
    "\n",
    "# Linearly decay the clipping parameter to zero\n",
    "def clip_eps_setter(env, agent, value):\n",
    "    agent.clip_eps = value\n",
    "\n",
    "\n",
    "def clip_action_filter(a):\n",
    "    return np.clip(a, action_space.low, action_space.high)\n",
    "\n",
    "def reward_filter(r):\n",
    "    return r\n",
    "\n",
    "\n",
    "def phi(obs):\n",
    "    obs=np.array(obs)\n",
    "    return obs.astype(np.float32)\n",
    "\n",
    "\n",
    "def make_env(test,render=False):\n",
    "    env = ProstheticsEnv(visualize=render)\n",
    "    # Use different random seeds for train and test envs\n",
    "    env_seed = 2 ** 32 - 1 - seed if test else seed\n",
    "    env.seed(env_seed)\n",
    "\n",
    "    if not test:\n",
    "        misc.env_modifiers.make_reward_filtered(env, reward_filter)\n",
    "    if render and not test:\n",
    "        misc.env_modifiers.make_rendered(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed used in ChainerRL\n",
    "misc.set_random_seed(seed)\n",
    "env = make_env(test=False,render=False)\n",
    "obs_space=env.observation_space\n",
    "obs_size = obs_space.low.size\n",
    "action_space = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A3CFFGaussian(obs_size, action_space,bound_mean=True,normalize_obs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting optimizer function adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = chainer.optimizers.Adam(alpha=actor_lr, eps=1e-5)\n",
    "opt.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent (algorithm) function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO(model, opt,\n",
    "                #gpu=args.gpu,\n",
    "                phi=phi,\n",
    "                update_interval=update_interval,\n",
    "                minibatch_size=batch_size, epochs=epochs,\n",
    "                clip_eps_vf=None, entropy_coef=entropy_coef,\n",
    "                #standardize_advantages=args.standardize_advantages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = make_env(test=True,render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, save reward in Text file and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=[]\n",
    "G_mean=[]\n",
    "for ep in range(1, number_of_episodes+ 1):\n",
    "    if ep%100:\n",
    "        agent.save(\"PPO_Prosthetic_5000\")\n",
    "    obs = env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    episode_rewards=[]\n",
    "    while not done and t < max_episode_length:\n",
    "        # Uncomment to watch the behaviour\n",
    "        #env.render()\n",
    "        action = agent.act_and_train(obs, reward)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        episode_rewards.append(reward)\n",
    "        t += 1\n",
    "        \n",
    "    if done or t >= max_episode_length :\n",
    "            \n",
    "            # Calculate sum of the rewards\n",
    "        episode_rewards_sum = sum(episode_rewards)     \n",
    "        G.append(episode_rewards_sum)\n",
    "        total_G = np.sum(G)\n",
    "        maximumReturn = np.amax(G)\n",
    "        print(\"%f\" % (episode_rewards_sum), file=open(\"PPO_Prosthetic_5000.txt\", \"a\"))\n",
    "        if ep % 10 == 0:\n",
    "                \n",
    "            print(\"==========================================\")\n",
    "            print(\"Episode: \", ep)\n",
    "            print(\"Rewards: \", episode_rewards_sum)\n",
    "            print(\"Max reward so far: \", maximumReturn)\n",
    "            # Mean reward\n",
    "            total_reward_mean = np.divide(total_G, ep+1)\n",
    "            G_mean.append(total_reward_mean)\n",
    "            print(\"Mean Reward\", total_reward_mean)\n",
    "            print(\"%f\" % (total_reward_mean), file=open(\"PPO_MEAN_Prosthetic_5000.txt\", \"a\"))\n",
    "                     \n",
    "    agent.stop_episode_and_train(obs, reward, done)\n",
    "    \n",
    "    \n",
    "print('Finished.')\n",
    "plt.xlabel('episdes')\n",
    "plt.ylabel('reword')\n",
    "plt.plot(G)   \n",
    "plt.savefig('PPO_prosthetic_5000', dpi = 1000)\n",
    "\n",
    "\n",
    "plt.plot(G_mean)\n",
    "plt.ylabel('Average of Returns')\n",
    "plt.xlabel('Number of episodes/10')\n",
    "\n",
    "plt.savefig(\"ReturnsAverage_VS_Episodes_PPO_prosthetic_5000\", dpi = 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
