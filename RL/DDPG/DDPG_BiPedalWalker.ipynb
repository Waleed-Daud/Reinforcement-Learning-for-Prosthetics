{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper libraries\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()  # NOQA\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "#___________________________________________________________________\n",
    "# Import scientific libraries\n",
    "\n",
    "import numpy as np               # Numpy, a good library to deal with matrices in python.\n",
    "import matplotlib.pyplot as plt  # Matplotlib, a good library for plotting in python.\n",
    "from matplotlib import style\n",
    "#___________________________________________________________________\n",
    "import gym                       # Gym, a collection of RL environemnts.\n",
    "gym.undo_logger_setup()  # NOQA\n",
    "from gym import spaces  \n",
    "import gym.wrappers\n",
    "\n",
    "#___________________________________________________________________\n",
    "\n",
    "import chainer                               # Chainer, a python-based deep learning framework. Chainerrl, a reinforcement learning library based on chainer framework.\n",
    "from chainer import optimizers               # a collection of Neural Network optimizers.\n",
    "from chainerrl.agents.ddpg import DDPG       # a DDPG agent\n",
    "from chainerrl.agents.ddpg import DDPGModel  # a DDPG model, responsibles to combine the policy network and the value function network.\n",
    "from chainerrl import explorers              # a collection of explores functions.\n",
    "from chainerrl import misc                   # a collection of utility functions to manipulate the environemnts.\n",
    "from chainerrl import policy                 # a policy network\n",
    "from chainerrl import q_functions            # a value function network\n",
    "from chainerrl import replay_buffer          # a Replay buffer to store a set of observations for the DDPG agent.\n",
    "\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment settings\n",
    "\n",
    "env_name='BipedalWalker-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chainer's settings\n",
    "seed=0\n",
    "gpu=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters for Policy and Value function Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters: 1\n",
    "\n",
    "\n",
    "# Policy Network ( Actor ).\n",
    "\n",
    "actor_hidden_layers=3                        # Number of hidden layers.\n",
    "actor_hidden_units=300                       # Number of hidden units\n",
    "actor_lr=1e-4                                # learning rate\n",
    "\n",
    "# Value function Network ( Critic )\n",
    "critic_hidden_layers=3                       # Number of hidden layers.\n",
    "critic_hidden_units=300                      # Number of hidden units\n",
    "critic_lr=1e-3                               # learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters: 2\n",
    "\n",
    "number_of_episodes=2000                     # Number of episodes\n",
    "max_episode_length=1000                     # Max length of the episode.\n",
    "\n",
    "replay_buffer_size=5 * 10 ** 5              # the size of the replay buffer.\n",
    "replay_start_size=5000                      # the size of the replay buffer when the network starts the training step.\n",
    "number_of_update_times=1                    # Number of repetition of update.\n",
    "\n",
    "target_update_interval=1                    # Target update interval in each step.\n",
    "target_update_method='soft'                 # the type of update: hard or soft.\n",
    "\n",
    "soft_update_tau=1e-2                        # The value of Tau  in the soft target update.\n",
    "update_interval=4                           # Model update interval in eac step.\n",
    "number_of_eval_runs=100\n",
    "# eval_interval=10 ** 5\n",
    "\n",
    "# final_exploration_steps=10 ** 6            \n",
    "\n",
    "gamma=0.995                               # Discount factor\n",
    "minibatch_size=128                        # Batch size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A set of helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper's functions\n",
    "\n",
    "def clip_action_filter(a):\n",
    "    \"\"\" limit the an action value between the higest and lowest values in action space.\n",
    "    Input: a\n",
    "    Output: clipped action\n",
    "    \"\"\"\n",
    "    return np.clip(a, action_space.low, action_space.high)\n",
    "\n",
    "def reward_filter(r):\n",
    "    \"\"\" Scale the reward value.\n",
    "    Input: reward (r)\n",
    "    Output: scaled reward\n",
    "    \"\"\"\n",
    "    return r *1 #1e-2\n",
    "\n",
    "\n",
    "def phi(obs):\n",
    "    \"\"\" Convert the data type of the observation to float-32\n",
    "    Input: observation (obs)\n",
    "    Output:  the processed observation \n",
    "    \"\"\" \n",
    "    obs=np.array(obs)\n",
    "    return obs.astype(np.float32)\n",
    "\n",
    "\n",
    "def random_action():\n",
    "    \"\"\" Generate a random action.\n",
    "    Input: None\n",
    "    Output:  a random action\n",
    "    \"\"\" \n",
    "    a = action_space.sample()\n",
    "    if isinstance(a, np.ndarray):\n",
    "        a = a.astype(np.float32)\n",
    "    return a\n",
    "\n",
    "\n",
    "def make_env(test,env_name,render=False):\n",
    "     \"\"\" Create an instance from \"CartPole\" environment\n",
    "    Input: a boolean value to show if it's an agent training experiment or test experiment (test)\n",
    "    Output:  \"CartPole\" environment (env)\n",
    "    \"\"\" \n",
    "        \n",
    "    env = gym.make(env_name)\n",
    "    # Use different random seeds for train and test envs\n",
    "    env_seed = 2 ** 32 - 1 - seed if test else seed\n",
    "    env.seed(env_seed)\n",
    "    #if args.monitor:\n",
    "        #env = gym.wrappers.Monitor(env, args.outdir)\n",
    "    if isinstance(env.action_space, spaces.Box):\n",
    "        misc.env_modifiers.make_action_filtered(env, clip_action_filter)\n",
    "    if not test:\n",
    "        misc.env_modifiers.make_reward_filtered(env, reward_filter)\n",
    "    if render and not test:\n",
    "        misc.env_modifiers.make_rendered(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed used in ChainerRL\n",
    "misc.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(test=False,env_name=env_name,render=False)\n",
    "timestep_limit = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "obs_size = np.asarray(env.observation_space.shape).prod()\n",
    "action_space = env.action_space\n",
    "\n",
    "action_size = np.asarray(action_space.shape).prod()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the \"Actor\" and \"Value function\" Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Network\n",
    "\n",
    "q_func = q_functions.FCSAQFunction(\n",
    "            obs_size, \n",
    "            action_size,\n",
    "            n_hidden_channels=critic_hidden_units,\n",
    "            n_hidden_layers=critic_hidden_layers)\n",
    "\n",
    "# policy Network\n",
    "\n",
    "pi = policy.FCDeterministicPolicy(\n",
    "            obs_size, \n",
    "            action_size=action_size,\n",
    "            n_hidden_channels=actor_hidden_units,\n",
    "            n_hidden_layers=actor_hidden_layers,\n",
    "            min_action=action_space.low, \n",
    "            max_action=action_space.high,\n",
    "            bound_action=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the \"DDPG\" agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Model\n",
    "\n",
    "model = DDPGModel(q_func=q_func, policy=pi)\n",
    "opt_actor = optimizers.Adam(alpha=actor_lr)\n",
    "opt_critic = optimizers.Adam(alpha=critic_lr)\n",
    "opt_actor.setup(model['policy'])\n",
    "opt_critic.setup(model['q_function'])\n",
    "opt_actor.add_hook(chainer.optimizer.GradientClipping(1.0), 'hook_a')\n",
    "opt_critic.add_hook(chainer.optimizer.GradientClipping(1.0), 'hook_c')\n",
    "\n",
    "rbuf = replay_buffer.ReplayBuffer(replay_buffer_size)\n",
    "ou_sigma = (action_space.high - action_space.low) * 0.2\n",
    "\n",
    "explorer = explorers.AdditiveOU(sigma=ou_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The agent\n",
    "agent = DDPG(model, opt_actor, opt_critic, rbuf, gamma=gamma,\n",
    "                 explorer=explorer, replay_start_size=replay_start_size,\n",
    "                 target_update_method=target_update_method,\n",
    "                 target_update_interval=target_update_interval,\n",
    "                 update_interval=update_interval,\n",
    "                 soft_update_tau=soft_update_tau,\n",
    "                 n_times_update=number_of_update_times,\n",
    "                 phi=phi,minibatch_size=minibatch_size\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=[]\n",
    "G_mean=[]\n",
    "for ep in range(1, number_of_episodes+ 1):\n",
    "    if ep%100:\n",
    "        agent.save(\"DDPG_Walker2D_10000\")\n",
    "    obs = env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    episode_rewards=[]\n",
    "    while not done and t < max_episode_length:\n",
    "        # Uncomment to watch the behaviour\n",
    "        #env.render()\n",
    "        action = agent.act_and_train(obs, reward)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        episode_rewards.append(reward)\n",
    "        t += 1\n",
    "        \n",
    "    if done or t >= max_episode_length :\n",
    "            \n",
    "            # Calculate sum of the rewards\n",
    "        episode_rewards_sum = sum(episode_rewards)     \n",
    "        G.append(episode_rewards_sum)\n",
    "        total_G = np.sum(G)\n",
    "        maximumReturn = np.amax(G)\n",
    "        print(\"%f\" % (episode_rewards_sum), file=open(\"DDPG_Walker2D_reward_10000.txt\", \"a\"))\n",
    "        if ep % 10 == 0:\n",
    "                \n",
    "            print(\"==========================================\")\n",
    "            print(\"Episode: \", ep)\n",
    "            print(\"Rewards: \", episode_rewards_sum)\n",
    "            print(\"Max reward so far: \", maximumReturn)\n",
    "            # Mean reward\n",
    "            total_reward_mean = np.divide(total_G, ep+1)\n",
    "            G_mean.append(total_reward_mean)\n",
    "            print(\"Mean Reward\", total_reward_mean)\n",
    "            print(\"%f\" % (total_reward_mean), file=open(\"DDPG_Walker2D_MEAN_Reward_10000.txt\", \"a\"))    \n",
    "                \n",
    "    agent.stop_episode_and_train(obs, reward, done)\n",
    "    \n",
    "    \n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"DDPG_BiPedalWalker_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(G)\n",
    "plt.ylabel('Returns')\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.savefig(\"Returns_VS_Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(G_mean)\n",
    "plt.ylabel('Average of Returns ')\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.savefig(\"ReturnsAverage_VS_Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (OpenSimEnv_V2)",
   "language": "python",
   "name": "opensimenv_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
