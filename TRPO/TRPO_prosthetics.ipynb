{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRPO in OpenSim environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libararies (OpenSim, ChainerRL, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()  # NOQA\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "\n",
    "from osim.env import ProstheticsEnv\n",
    "\n",
    "import chainer\n",
    "from chainer import functions as F\n",
    "from chainerrl.agents.trpo import TRPO\n",
    "from chainerrl.agents.acer import ACERSeparateModel\n",
    "from chainerrl.optimizers.nonbias_weight_decay import NonbiasWeightDecay\n",
    "from chainerrl.optimizers import rmsprop_async\n",
    "from chainerrl.action_value import QuadraticActionValue #Q-function output for continuous action space\n",
    "#from chainerrl.action_value import DiscreteActionValue\n",
    "import chainerrl\n",
    "from chainerrl import experiments\n",
    "from chainerrl import explorers\n",
    "from chainerrl import misc\n",
    "from chainerrl import policy\n",
    "from chainerrl import q_functions\n",
    "from chainerrl import replay_buffer\n",
    "from chainerrl import v_functions\n",
    "\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For all experiments we use the same random seed\n",
    "    GPU =-1 if you use CPU \n",
    "    GPU = 1 if you use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=0\n",
    "gpu=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyper-parameters, Numbers of episodes and timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_episodes=5000\n",
    "max_episode_length=1000 # number of timesteps per episode\n",
    "\n",
    "replay_start_size=5000\n",
    "number_of_update_times=1\n",
    "\n",
    "target_update_interval=1\n",
    "target_update_method='soft'\n",
    "\n",
    "soft_update_tau=1e-2\n",
    "update_interval=4\n",
    "number_of_eval_runs=100\n",
    "eval_interval=10 ** 5\n",
    "\n",
    "final_exploration_steps=10 ** 6\n",
    "\n",
    "gamma=0.995\n",
    "minibatch_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining reward, environment, random actions functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper's functions\n",
    "\n",
    "def clip_action_filter(a):\n",
    "    return np.clip(a, action_space.low, action_space.high)\n",
    "\n",
    "def reward_filter(r):\n",
    "    return r \n",
    "\n",
    "\n",
    "def phi(obs):\n",
    "    obs=np.array(obs)\n",
    "    return obs.astype(np.float32)\n",
    "\n",
    "def random_action():\n",
    "    a = action_space.sample()\n",
    "    if isinstance(a, np.ndarray):\n",
    "        a = a.astype(np.float32)\n",
    "    return a\n",
    "\n",
    "\n",
    "def make_env(test,render=False):\n",
    "    env = ProstheticsEnv(visualize=render)\n",
    "    # Use different random seeds for train and test envs\n",
    "    env_seed = 2 ** 32 - 1 - seed if test else seed\n",
    "    env.seed(env_seed)\n",
    "    #if args.monitor:\n",
    "        #env = gym.wrappers.Monitor(env, args.outdir)\n",
    "    if isinstance(env.action_space, spaces.Box):\n",
    "        misc.env_modifiers.make_action_filtered(env, clip_action_filter)\n",
    "    if not test:\n",
    "        misc.env_modifiers.make_reward_filtered(env, reward_filter)\n",
    "    if render and not test:\n",
    "        misc.env_modifiers.make_rendered(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed used in ChainerRL\n",
    "misc.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(test=False,render=False) # render=True for visualisation \n",
    "obs_size = np.asarray(env.observation_space.shape).prod()\n",
    "action_space = env.action_space\n",
    "\n",
    "action_size = np.asarray(action_space.shape).prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Value function, policy function and optimizer function adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_space = env.observation_space\n",
    "# Value function \n",
    "v_fuc = chainerrl.v_functions.FCVFunction(\n",
    "        obs_space.low.size,\n",
    "        n_hidden_channels=64,\n",
    "        n_hidden_layers=4,\n",
    "        last_wscale=0.01,\n",
    "        nonlinearity=F.tanh,\n",
    ")\n",
    "\n",
    "\n",
    "v_fuc_opt = chainer.optimizers.Adam()\n",
    "v_fuc_opt.setup(v_fuc)\n",
    "\n",
    "# Policy function \n",
    "pi =  chainerrl.policies.FCGaussianPolicyWithStateIndependentCovariance(\n",
    "                obs_space.low.size,\n",
    "                action_space.low.size,\n",
    "                n_hidden_channels=64,\n",
    "                n_hidden_layers=3,\n",
    "                mean_wscale=0.01,\n",
    "                nonlinearity=F.tanh,\n",
    "                var_type='diagonal',\n",
    "                var_func=lambda x: F.exp(2 * x),  # Parameterize log std\n",
    "                var_param_init=0,  # log std = 0 => std = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbuf = replay_buffer.ReplayBuffer(5 * 10)\n",
    "\n",
    "ou_sigma = (action_space.high - action_space.low) * 0.2\n",
    "\n",
    "explorer = explorers.AdditiveOU(sigma=ou_sigma)\n",
    "#obs_normalizer = chainerrl.links.MLPConvolution2D(obs_space.low.size)\n",
    "obs_normalizer = chainerrl.links.EmpiricalNormalization(obs_space.low.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent (algorithm) function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The agent\n",
    "agent = TRPO(\n",
    "            policy=pi,\n",
    "                 vf=v_fuc,\n",
    "                 vf_optimizer = v_fuc_opt,\n",
    "                 obs_normalizer=obs_normalizer,\n",
    "                phi=lambda x: np.array(x).astype(np.float32, copy=False),\n",
    "                update_interval=update_interval,\n",
    "                conjugate_gradient_max_iter=100,\n",
    "                conjugate_gradient_damping=1e-3,\n",
    "                gamma=gamma,\n",
    "                lambd=0.97,\n",
    "                vf_epochs=5,\n",
    "                entropy_coef=0,\n",
    "            )\n",
    "#agent.load(\"TRPO_Prosthetic_model_edited_parameters_anther 2000_V2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, save reward in Text file and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=[]\n",
    "G_mean=[]\n",
    "for ep in range(1, number_of_episodes+ 1):\n",
    "    if ep%100:\n",
    "        agent.save(\"TRPO_Prosthetic_5000\")\n",
    "    obs = env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    episode_rewards=[]\n",
    "    while not done and t < max_episode_length:\n",
    "        # Uncomment to watch the behaviour\n",
    "        env.render()\n",
    "        action = agent.act_and_train(obs, reward)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        episode_rewards.append(reward)\n",
    "        t += 1\n",
    "        \n",
    "    if done or t >= max_episode_length :\n",
    "            \n",
    "            # Calculate sum of the rewards\n",
    "        episode_rewards_sum = sum(episode_rewards)     \n",
    "        G.append(episode_rewards_sum)\n",
    "        total_G = np.sum(G)\n",
    "        maximumReturn = np.amax(G)\n",
    "        print(\"%f\" % (episode_rewards_sum), file=open(\"TRPO_Prosthetic_5000.txt\", \"a\"))\n",
    "        #print(\"%i\" % (episode_rewards_sum))\n",
    "        if ep % 10 == 0:\n",
    "                \n",
    "            print(\"==========================================\")\n",
    "            print(\"Episode: \", ep)\n",
    "            print(\"Rewards: \", episode_rewards_sum)\n",
    "            print(\"Max reward so far: \", maximumReturn)\n",
    "            # Mean reward\n",
    "            total_reward_mean = np.divide(total_G, ep+1)\n",
    "            G_mean.append(total_reward_mean)\n",
    "            print(\"Mean Reward\", total_reward_mean)\n",
    "            print(\"%f\" % (total_reward_mean), file=open(\"TRPO_MEAN_Prosthetic_5000.txt\", \"a\"))    \n",
    "                \n",
    "    agent.stop_episode_and_train(obs, reward, done)\n",
    "    \n",
    "    \n",
    "print('Finished.')\n",
    "\n",
    "plt.xlabel('episdes')\n",
    "plt.ylabel('reword')\n",
    "plt.plot(G)   \n",
    "plt.savefig('trpo_prosthetic_5000', dpi = 1000)\n",
    "\n",
    "\n",
    "plt.plot(G_mean)\n",
    "plt.ylabel('Average of Returns')\n",
    "plt.xlabel('Number of episodes/10')\n",
    "\n",
    "plt.savefig(\"ReturnsAverage_VS_Episodes trpo_prosthetic_5000\", dpi = 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
